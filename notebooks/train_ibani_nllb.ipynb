{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåç Ibani-English NLLB Translation Model Training\n",
    "\n",
    "This notebook fine-tunes Meta's NLLB-200 model for Ibani ‚Üî English translation.\n",
    "\n",
    "**Steps:**\n",
    "1. Setup environment and install dependencies\n",
    "2. Upload training data\n",
    "3. Fine-tune NLLB-200 with LoRA\n",
    "4. Test the model\n",
    "5. Download the trained model\n",
    "\n",
    "**Runtime:** GPU (T4 or better recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers datasets accelerate peft bitsandbytes sentencepiece sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Upload Training Data\n",
    "\n",
    "Upload your `ibani_eng.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload training data\n",
    "print(\"üì§ Please upload your ibani_eng.json file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Get the filename\n",
    "data_file = list(uploaded.keys())[0]\n",
    "print(f\"\\n‚úì Uploaded: {data_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect data\n",
    "with open(data_file, 'r', encoding='utf-8') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "print(f\"‚úì Loaded {len(training_data)} training examples\\n\")\n",
    "print(\"Sample data:\")\n",
    "for i, item in enumerate(training_data[:3]):\n",
    "    print(f\"\\n{i+1}. Ibani: {item['ibani']}\")\n",
    "    print(f\"   English: {item['english']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"facebook/nllb-200-distilled-600M\"\n",
    "OUTPUT_DIR = \"ibani-nllb-model\"\n",
    "\n",
    "# Training hyperparameters\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-4\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "print(\"üìã Training Configuration:\")\n",
    "print(f\"  Base Model: {MODEL_NAME}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bidirectional dataset\n",
    "examples = []\n",
    "\n",
    "for item in training_data:\n",
    "    # Ibani ‚Üí English\n",
    "    examples.append({\n",
    "        'source': item['ibani'],\n",
    "        'target': item['english'],\n",
    "    })\n",
    "    \n",
    "    # English ‚Üí Ibani\n",
    "    examples.append({\n",
    "        'source': item['english'],\n",
    "        'target': item['ibani'],\n",
    "    })\n",
    "\n",
    "print(f\"‚úì Created {len(examples)} bidirectional examples\")\n",
    "\n",
    "# Convert to Dataset\n",
    "dataset = Dataset.from_list(examples)\n",
    "print(f\"‚úì Dataset created with {len(dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(f\"üì• Loading tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(\"‚úì Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(f\"üì• Loading model: {MODEL_NAME}\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"‚úì Model loaded\")\n",
    "print(f\"‚úì Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Setup LoRA for Efficient Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize examples.\"\"\"\n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        examples['source'],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    labels = tokenizer(\n",
    "        examples['target'],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize dataset\n",
    "print(\"üîÑ Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "print(\"‚úì Dataset tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/eval\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']\n",
    "\n",
    "print(f\"‚úì Train examples: {len(train_dataset)}\")\n",
    "print(f\"‚úì Eval examples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "print(\"‚úì Training configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Train the Model üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèãÔ∏è Starting Training...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Training Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "print(f\"üíæ Saving model to {OUTPUT_DIR}...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# Save training info\n",
    "training_info = {\n",
    "    \"base_model\": MODEL_NAME,\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"num_training_examples\": len(training_data),\n",
    "    \"num_bidirectional_examples\": len(examples),\n",
    "    \"use_lora\": True,\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/training_info.json\", 'w') as f:\n",
    "    json.dump(training_info, f, indent=2)\n",
    "\n",
    "print(\"‚úì Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test translation function\n",
    "def translate(text, max_length=200, num_beams=5):\n",
    "    \"\"\"Translate text using the trained model.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3,\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test with some examples\n",
    "print(\"üß™ Testing translations:\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_examples = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"My name is William\",\n",
    "    \"Thank you\",\n",
    "    \"Good morning\",\n",
    "]\n",
    "\n",
    "for text in test_examples:\n",
    "    translation = translate(text)\n",
    "    print(f\"\\nEnglish: {text}\")\n",
    "    print(f\"Ibani:   {translation}\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Download the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file of the model\n",
    "print(\"üì¶ Creating model archive...\")\n",
    "\n",
    "zip_filename = \"ibani_nllb_model.zip\"\n",
    "\n",
    "!zip -r {zip_filename} {OUTPUT_DIR}\n",
    "\n",
    "print(f\"\\n‚úì Model archived to {zip_filename}\")\n",
    "print(f\"‚úì Size: {Path(zip_filename).stat().st_size / (1024*1024):.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model\n",
    "print(\"üì• Downloading model...\")\n",
    "files.download(zip_filename)\n",
    "print(\"\\n‚úÖ Download started! Check your browser's download folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Next Steps\n",
    "\n",
    "1. **Download** the `ibani_nllb_model.zip` file\n",
    "2. **Extract** it to your local project's `models/` directory\n",
    "3. **Run** the FastAPI server: `python app.py`\n",
    "4. **Test** the API at `http://localhost:8080/docs`\n",
    "\n",
    "### Optional: Push to Hugging Face Hub\n",
    "\n",
    "To share your model:\n",
    "\n",
    "```python\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face\n",
    "login()\n",
    "\n",
    "# Push model\n",
    "model.push_to_hub(\"your-username/ibani-nllb-translator\")\n",
    "tokenizer.push_to_hub(\"your-username/ibani-nllb-translator\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
